{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569b6940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\witlab\\anaconda3\\envs\\gpu_test_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\witlab\\anaconda3\\envs\\gpu_test_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\witlab\\anaconda3\\envs\\gpu_test_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\witlab\\anaconda3\\envs\\gpu_test_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\witlab\\anaconda3\\envs\\gpu_test_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\witlab\\anaconda3\\envs\\gpu_test_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 222, 222, 64)  1792        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 111, 111, 64)  0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 109, 109, 128) 73856       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, 54, 54, 128)   0           conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 52, 52, 256)   295168      max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, 26, 26, 256)   0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 24, 24, 512)   1180160     max_pooling2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, 12, 12, 512)   0           conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 10, 10, 512)   2359808     max_pooling2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)   (None, 5, 5, 512)     0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 2)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 3, 3, 512)     2359808     max_pooling2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           384         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)   (None, 1, 1, 512)     0           conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 64)            8256        dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 512)           0           max_pooling2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 32)            2080        dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 256)           131328      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 16)            528         dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 272)           0           dense_1[0][0]                    \n",
      "                                                                   dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 12)            3276        concatenate_1[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 6,416,444\n",
      "Trainable params: 6,416,444\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, AveragePooling2D, Input, concatenate\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def create_cnn(input_shape):\n",
    "    # Define the CNN architecture\n",
    "    input_img = Input(shape=input_shape)\n",
    "    x = Conv2D(64, kernel_size=(3, 3), activation='relu')(input_img)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(256, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(512, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(512, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(512, kernel_size=(3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    cnn_output = Dense(256, activation='relu')(x)\n",
    "    \n",
    "    # Create and compile the CNN model\n",
    "    cnn_model = Model(inputs=input_img, outputs=cnn_output)\n",
    "    cnn_model.compile(optimizer='adam', loss='mean_absolute_error') # You can adjust the optimizer and loss function as needed\n",
    "    return cnn_model\n",
    "\n",
    "def create_dnn(input_shape):\n",
    "    # Define the DNN architecture\n",
    "    input_non_img = Input(shape=input_shape)\n",
    "    x = Dense(128, activation='relu')(input_non_img)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    dnn_output = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    # Create and compile the DNN model\n",
    "    dnn_model = Model(inputs=input_non_img, outputs=dnn_output)\n",
    "    dnn_model.compile(optimizer='adam', loss='mean_absolute_error') # You can adjust the optimizer and loss function as needed\n",
    "    return dnn_model\n",
    "\n",
    "def create_fusion_model(cnn_model, dnn_model):\n",
    "    # Concatenate the outputs of the CNN and DNN models\n",
    "    concatenated = concatenate([cnn_model.output, dnn_model.output])\n",
    "    output = Dense(12, activation='relu')(concatenated) \n",
    "\n",
    "    \n",
    "    # Create the fusion model\n",
    "    fusion_model = Model(inputs=[cnn_model.input, dnn_model.input], outputs=output)\n",
    "    fusion_model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['accuracy']) # Adjust loss and metrics as needed\n",
    "    return fusion_model\n",
    "\n",
    "# Define input shapes\n",
    "height, width, channels = 224, 224, 3 # Example image dimensions\n",
    "num_features = 2 # Example number of non-image features\n",
    "\n",
    "# Create CNN and DNN models\n",
    "cnn_model = create_cnn((height, width, channels))\n",
    "dnn_model = create_dnn((num_features,))\n",
    "\n",
    "# Create fusion model\n",
    "fusion_model = create_fusion_model(cnn_model, dnn_model)\n",
    "\n",
    "# Print model summary\n",
    "fusion_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c456e969",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T00:57:26.545315400Z",
     "start_time": "2024-04-17T00:57:22.106905300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17490846632963709809\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 78607156\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 8748432032109430883\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "print(tf.test.is_gpu_available())\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1d7295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8350, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# image_path = r'image\\auto_intg\\preprocess\\train_image_20240228'\n",
    "image_path=r'../image\\bh_auto_intg\\preprocess_no_cut\\train_image_backup\\train_image_20240405'\n",
    "\n",
    "image_files = [f for f in os.listdir(image_path) if f.endswith('.jpg')]\n",
    "\n",
    "images = []\n",
    "for file in image_files:\n",
    "    img = cv2.imread(os.path.join(image_path, file))\n",
    "    if img is None:\n",
    "        print(f\"{file} 읽을 수 없음\")\n",
    "        continue\n",
    "\n",
    "    img = img / 255.0  # 0~1로 정규화\n",
    "    images.append(img)\n",
    "\n",
    "images = np.array(images)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0cbb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = '../data/train_data_20240307-20240402_20240405_sun_info.xlsx'\n",
    "df = pd.read_excel(excel_path, engine='openpyxl')\n",
    "\n",
    "# 12개 지점 조도 데이터\n",
    "illum_columns = [f'new_illum_{i}' for i in [10, 11, 12, 4, 5, 6, 1, 2, 3, 7, 8, 9]]\n",
    "\n",
    "# 태양 고도각, 방위각 데이터\n",
    "solar_features_columns= df[['solar_elevation', 'solar_azimuth']]\n",
    "\n",
    "# numpy로 변경\n",
    "illum_data = (df[illum_columns]).to_numpy()\n",
    "solar_features_data = solar_features_columns.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8320ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train shape: (6680, 224, 224, 3)\n",
      "image_val shape: (1670, 224, 224, 3)\n",
      "sum_info_train shape: (6680, 2)\n",
      "sum_info_val shape: (1670, 2)\n",
      "illum_train shape: (6680, 12)\n",
      "illum_val shape: (1670, 12)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_train, image_val, sum_info_train, sum_info_val, illum_train, illum_val  = train_test_split(images, solar_features_data, illum_data, test_size=0.2, random_state=42)\n",
    "print(\"image_train shape:\", image_train.shape)\n",
    "print(\"image_val shape:\", image_val.shape)\n",
    "print(\"sum_info_train shape:\", sum_info_train.shape)\n",
    "print(\"sum_info_val shape:\", sum_info_val.shape)\n",
    "print(\"illum_train shape:\", illum_train.shape)\n",
    "print(\"illum_val shape:\", illum_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7543973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 1670 samples\n",
      "Epoch 1/500\n",
      "6680/6680 [==============================] - 462s - loss: 85.3542 - acc: 0.8094 - val_loss: 63.2474 - val_acc: 0.8623\n",
      "Epoch 2/500\n",
      "6680/6680 [==============================] - 152s - loss: 53.7242 - acc: 0.8521 - val_loss: 74.0684 - val_acc: 0.88325\n",
      "Epoch 3/500\n",
      "6680/6680 [==============================] - 153s - loss: 48.8404 - acc: 0.8807 - val_loss: 38.3119 - val_acc: 0.9030\n",
      "Epoch 4/500\n",
      "6680/6680 [==============================] - 152s - loss: 42.1391 - acc: 0.9043 - val_loss: 36.6118 - val_acc: 0.9132\n",
      "Epoch 5/500\n",
      "6680/6680 [==============================] - 152s - loss: 41.0142 - acc: 0.9108 - val_loss: 33.1794 - val_acc: 0.9126\n",
      "Epoch 6/500\n",
      "6680/6680 [==============================] - 152s - loss: 38.1565 - acc: 0.9145 - val_loss: 36.4281 - val_acc: 0.9198\n",
      "Epoch 7/500\n",
      "6680/6680 [==============================] - 153s - loss: 35.3949 - acc: 0.9238 - val_loss: 31.7911 - val_acc: 0.9096\n",
      "Epoch 8/500\n",
      "6680/6680 [==============================] - 153s - loss: 33.4883 - acc: 0.9181 - val_loss: 28.6997 - val_acc: 0.9317\n",
      "Epoch 9/500\n",
      "6680/6680 [==============================] - 152s - loss: 31.3959 - acc: 0.9243 - val_loss: 25.7282 - val_acc: 0.9216\n",
      "Epoch 10/500\n",
      "6680/6680 [==============================] - 151s - loss: 29.1720 - acc: 0.9228 - val_loss: 26.2989 - val_acc: 0.8988\n",
      "Epoch 11/500\n",
      "6680/6680 [==============================] - 151s - loss: 27.7004 - acc: 0.9253 - val_loss: 23.4983 - val_acc: 0.9305\n",
      "Epoch 12/500\n",
      "6680/6680 [==============================] - 151s - loss: 26.1522 - acc: 0.9278 - val_loss: 20.7488 - val_acc: 0.9299\n",
      "Epoch 13/500\n",
      "6680/6680 [==============================] - 151s - loss: 24.5952 - acc: 0.9314 - val_loss: 22.7769 - val_acc: 0.9293\n",
      "Epoch 14/500\n",
      "6680/6680 [==============================] - 150s - loss: 23.9324 - acc: 0.9286 - val_loss: 36.7957 - val_acc: 0.9449\n",
      "Epoch 15/500\n",
      "6680/6680 [==============================] - 150s - loss: 23.0745 - acc: 0.9314 - val_loss: 21.7218 - val_acc: 0.9419\n",
      "Epoch 16/500\n",
      "6680/6680 [==============================] - 150s - loss: 22.2549 - acc: 0.9293 - val_loss: 30.7098 - val_acc: 0.9425\n",
      "Epoch 17/500\n",
      "6680/6680 [==============================] - 151s - loss: 21.3456 - acc: 0.9353 - val_loss: 18.7933 - val_acc: 0.9419\n",
      "Epoch 18/500\n",
      "6680/6680 [==============================] - 151s - loss: 21.8076 - acc: 0.9328 - val_loss: 22.6505 - val_acc: 0.9335\n",
      "Epoch 19/500\n",
      "6680/6680 [==============================] - 151s - loss: 20.7475 - acc: 0.9353 - val_loss: 19.0487 - val_acc: 0.9305\n",
      "Epoch 20/500\n",
      "6680/6680 [==============================] - 151s - loss: 20.2704 - acc: 0.9365 - val_loss: 22.5053 - val_acc: 0.9395\n",
      "Epoch 21/500\n",
      "6680/6680 [==============================] - 151s - loss: 20.2760 - acc: 0.9374 - val_loss: 17.4663 - val_acc: 0.9275\n",
      "Epoch 22/500\n",
      "6680/6680 [==============================] - 151s - loss: 19.3044 - acc: 0.9373 - val_loss: 20.0958 - val_acc: 0.9371\n",
      "Epoch 23/500\n",
      "6680/6680 [==============================] - 151s - loss: 19.4096 - acc: 0.9379 - val_loss: 17.4686 - val_acc: 0.9413\n",
      "Epoch 24/500\n",
      "6680/6680 [==============================] - 150s - loss: 18.9672 - acc: 0.9328 - val_loss: 18.2871 - val_acc: 0.9449\n",
      "Epoch 25/500\n",
      "6680/6680 [==============================] - 150s - loss: 18.4003 - acc: 0.9365 - val_loss: 20.0061 - val_acc: 0.9353\n",
      "Epoch 26/500\n",
      "6680/6680 [==============================] - 151s - loss: 18.7534 - acc: 0.9341 - val_loss: 19.3113 - val_acc: 0.9263\n",
      "Epoch 27/500\n",
      "6680/6680 [==============================] - 150s - loss: 17.9702 - acc: 0.9382 - val_loss: 17.8872 - val_acc: 0.9335\n",
      "Epoch 28/500\n",
      "6680/6680 [==============================] - 150s - loss: 17.0651 - acc: 0.9392 - val_loss: 24.7710 - val_acc: 0.9473\n",
      "Epoch 29/500\n",
      "6680/6680 [==============================] - 150s - loss: 17.8632 - acc: 0.9391 - val_loss: 17.9915 - val_acc: 0.9371\n",
      "Epoch 30/500\n",
      "6680/6680 [==============================] - 151s - loss: 16.9002 - acc: 0.9364 - val_loss: 28.5567 - val_acc: 0.9257\n",
      "Epoch 31/500\n",
      "6680/6680 [==============================] - 150s - loss: 16.9350 - acc: 0.9377 - val_loss: 17.6793 - val_acc: 0.9401\n",
      "Epoch 32/500\n",
      "6680/6680 [==============================] - 151s - loss: 16.2134 - acc: 0.9433 - val_loss: 16.2770 - val_acc: 0.9491\n",
      "Epoch 33/500\n",
      "6680/6680 [==============================] - 150s - loss: 16.4064 - acc: 0.9413 - val_loss: 18.7804 - val_acc: 0.9485\n",
      "Epoch 34/500\n",
      "6680/6680 [==============================] - 150s - loss: 16.2949 - acc: 0.9436 - val_loss: 17.7834 - val_acc: 0.9491\n",
      "Epoch 35/500\n",
      "6680/6680 [==============================] - 150s - loss: 15.9698 - acc: 0.9412 - val_loss: 18.9855 - val_acc: 0.9467\n",
      "Epoch 36/500\n",
      "6680/6680 [==============================] - 150s - loss: 15.5425 - acc: 0.9413 - val_loss: 17.6038 - val_acc: 0.9431\n",
      "Epoch 37/500\n",
      "6680/6680 [==============================] - 150s - loss: 14.9920 - acc: 0.9406 - val_loss: 18.1880 - val_acc: 0.9467\n",
      "Epoch 38/500\n",
      "6680/6680 [==============================] - 150s - loss: 15.2259 - acc: 0.9449 - val_loss: 20.9503 - val_acc: 0.9347\n",
      "Epoch 39/500\n",
      "6680/6680 [==============================] - 150s - loss: 14.7930 - acc: 0.9443 - val_loss: 17.7249 - val_acc: 0.9557\n",
      "Epoch 40/500\n",
      "6680/6680 [==============================] - 150s - loss: 14.2350 - acc: 0.9469 - val_loss: 28.9258 - val_acc: 0.9443\n",
      "Epoch 41/500\n",
      "6680/6680 [==============================] - 150s - loss: 14.8071 - acc: 0.9440 - val_loss: 18.2485 - val_acc: 0.9437\n",
      "Epoch 42/500\n",
      "6680/6680 [==============================] - 150s - loss: 13.9548 - acc: 0.9431 - val_loss: 16.4528 - val_acc: 0.9539\n",
      "Epoch 43/500\n",
      "6680/6680 [==============================] - 151s - loss: 14.5592 - acc: 0.9454 - val_loss: 14.4005 - val_acc: 0.9473\n",
      "Epoch 44/500\n",
      "6680/6680 [==============================] - 150s - loss: 14.3029 - acc: 0.9446 - val_loss: 22.8042 - val_acc: 0.9437\n",
      "Epoch 45/500\n",
      "6680/6680 [==============================] - 150s - loss: 13.9415 - acc: 0.9443 - val_loss: 16.6139 - val_acc: 0.9455\n",
      "Epoch 46/500\n",
      "6680/6680 [==============================] - 150s - loss: 13.4438 - acc: 0.9433 - val_loss: 16.5010 - val_acc: 0.9539\n",
      "Epoch 47/500\n",
      "6680/6680 [==============================] - 150s - loss: 13.4193 - acc: 0.9509 - val_loss: 17.8685 - val_acc: 0.9533\n",
      "Epoch 48/500\n",
      "6680/6680 [==============================] - 151s - loss: 13.4231 - acc: 0.9513 - val_loss: 14.3861 - val_acc: 0.9449\n",
      "Epoch 49/500\n",
      "6680/6680 [==============================] - 150s - loss: 13.1149 - acc: 0.9485 - val_loss: 14.7980 - val_acc: 0.9551\n",
      "Epoch 50/500\n",
      "6680/6680 [==============================] - 151s - loss: 13.6625 - acc: 0.9515 - val_loss: 14.2134 - val_acc: 0.9533\n",
      "Epoch 51/500\n",
      "6680/6680 [==============================] - 150s - loss: 12.6268 - acc: 0.9516 - val_loss: 14.9785 - val_acc: 0.9473\n",
      "Epoch 52/500\n",
      "6680/6680 [==============================] - 150s - loss: 13.0700 - acc: 0.9473 - val_loss: 16.4071 - val_acc: 0.9455\n",
      "Epoch 53/500\n",
      "6680/6680 [==============================] - 150s - loss: 12.8314 - acc: 0.9491 - val_loss: 15.6128 - val_acc: 0.9461\n",
      "Epoch 54/500\n",
      "6680/6680 [==============================] - 150s - loss: 12.8659 - acc: 0.9476 - val_loss: 14.3550 - val_acc: 0.9461\n",
      "Epoch 55/500\n",
      "6680/6680 [==============================] - 151s - loss: 12.4996 - acc: 0.9470 - val_loss: 13.9389 - val_acc: 0.9509\n",
      "Epoch 56/500\n",
      "6680/6680 [==============================] - 150s - loss: 12.5830 - acc: 0.9494 - val_loss: 14.8897 - val_acc: 0.9563\n",
      "Epoch 57/500\n",
      "6680/6680 [==============================] - 150s - loss: 13.0262 - acc: 0.9528 - val_loss: 14.5142 - val_acc: 0.9515\n",
      "Epoch 58/500\n",
      "6680/6680 [==============================] - 150s - loss: 12.1695 - acc: 0.9539 - val_loss: 14.9261 - val_acc: 0.9581\n",
      "Epoch 59/500\n",
      "6680/6680 [==============================] - 150s - loss: 12.1882 - acc: 0.9488 - val_loss: 14.4543 - val_acc: 0.9431\n",
      "Epoch 60/500\n",
      "6680/6680 [==============================] - 150s - loss: 11.8970 - acc: 0.9527 - val_loss: 14.0854 - val_acc: 0.9473\n",
      "Epoch 61/500\n",
      "6680/6680 [==============================] - 150s - loss: 11.9875 - acc: 0.9509 - val_loss: 14.4832 - val_acc: 0.9515\n",
      "Epoch 62/500\n",
      "6680/6680 [==============================] - 151s - loss: 11.4781 - acc: 0.9496 - val_loss: 13.6807 - val_acc: 0.9467\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 150s - loss: 12.0122 - acc: 0.9442 - val_loss: 14.9619 - val_acc: 0.9461\n",
      "Epoch 64/500\n",
      "6680/6680 [==============================] - 151s - loss: 11.5998 - acc: 0.9519 - val_loss: 13.1455 - val_acc: 0.9485\n",
      "Epoch 65/500\n",
      "6680/6680 [==============================] - 151s - loss: 11.1393 - acc: 0.9500 - val_loss: 12.9524 - val_acc: 0.9539\n",
      "Epoch 66/500\n",
      "6680/6680 [==============================] - 151s - loss: 11.9646 - acc: 0.9500 - val_loss: 13.3926 - val_acc: 0.9599\n",
      "Epoch 67/500\n",
      "6680/6680 [==============================] - 150s - loss: 11.5999 - acc: 0.9545 - val_loss: 14.7404 - val_acc: 0.9455\n",
      "Epoch 68/500\n",
      "6680/6680 [==============================] - 151s - loss: 11.4544 - acc: 0.9527 - val_loss: 13.9067 - val_acc: 0.9569\n",
      "Epoch 69/500\n",
      "6680/6680 [==============================] - 150s - loss: 11.3539 - acc: 0.9518 - val_loss: 18.3905 - val_acc: 0.9479\n",
      "Epoch 70/500\n",
      "6680/6680 [==============================] - 150s - loss: 11.1609 - acc: 0.9522 - val_loss: 14.0865 - val_acc: 0.9569\n",
      "Epoch 71/500\n",
      "6680/6680 [==============================] - 151s - loss: 10.8707 - acc: 0.9557 - val_loss: 13.1279 - val_acc: 0.9521\n",
      "Epoch 72/500\n",
      "6680/6680 [==============================] - 150s - loss: 10.8776 - acc: 0.9548 - val_loss: 17.6965 - val_acc: 0.9611\n",
      "Epoch 73/500\n",
      "6680/6680 [==============================] - 151s - loss: 10.8986 - acc: 0.9531 - val_loss: 13.3728 - val_acc: 0.9575\n",
      "Epoch 74/500\n",
      "6680/6680 [==============================] - 150s - loss: 11.0381 - acc: 0.9518 - val_loss: 14.3441 - val_acc: 0.9509\n",
      "Epoch 75/500\n",
      "6680/6680 [==============================] - 151s - loss: 10.7886 - acc: 0.9561 - val_loss: 13.4140 - val_acc: 0.9635\n",
      "Epoch 76/500\n",
      "6680/6680 [==============================] - 150s - loss: 10.7125 - acc: 0.9543 - val_loss: 13.6005 - val_acc: 0.9593\n",
      "Epoch 77/500\n",
      "6680/6680 [==============================] - 151s - loss: 10.9367 - acc: 0.9563 - val_loss: 12.5564 - val_acc: 0.9545\n",
      "Epoch 78/500\n",
      "6680/6680 [==============================] - 150s - loss: 10.5721 - acc: 0.9539 - val_loss: 13.3827 - val_acc: 0.9569\n",
      "Epoch 79/500\n",
      "6680/6680 [==============================] - 151s - loss: 10.5527 - acc: 0.9506 - val_loss: 13.7503 - val_acc: 0.9629\n",
      "Epoch 80/500\n",
      "6680/6680 [==============================] - 150s - loss: 10.2431 - acc: 0.9519 - val_loss: 12.9555 - val_acc: 0.9569\n",
      "Epoch 81/500\n",
      "6680/6680 [==============================] - 151s - loss: 10.5858 - acc: 0.9548 - val_loss: 12.3938 - val_acc: 0.9473\n",
      "Epoch 82/500\n",
      "6680/6680 [==============================] - 150s - loss: 10.4899 - acc: 0.9537 - val_loss: 13.8121 - val_acc: 0.9581\n",
      "Epoch 83/500\n",
      "6680/6680 [==============================] - 150s - loss: 10.5188 - acc: 0.9533 - val_loss: 12.3324 - val_acc: 0.9623\n",
      "Epoch 84/500\n",
      "6680/6680 [==============================] - 150s - loss: 10.0498 - acc: 0.9537 - val_loss: 14.0969 - val_acc: 0.9563\n",
      "Epoch 85/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.9297 - acc: 0.9548 - val_loss: 13.1511 - val_acc: 0.9593\n",
      "Epoch 86/500\n",
      "6680/6680 [==============================] - 150s - loss: 10.1070 - acc: 0.9484 - val_loss: 17.9208 - val_acc: 0.9497\n",
      "Epoch 87/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.9725 - acc: 0.9539 - val_loss: 13.8655 - val_acc: 0.9467\n",
      "Epoch 88/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.9242 - acc: 0.9594 - val_loss: 14.6208 - val_acc: 0.9341\n",
      "Epoch 89/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.6697 - acc: 0.9522 - val_loss: 14.1300 - val_acc: 0.9569\n",
      "Epoch 90/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.9246 - acc: 0.9555 - val_loss: 12.4382 - val_acc: 0.9629\n",
      "Epoch 91/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.8594 - acc: 0.9554 - val_loss: 13.9115 - val_acc: 0.9479\n",
      "Epoch 92/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.6766 - acc: 0.9585 - val_loss: 17.0607 - val_acc: 0.9389\n",
      "Epoch 93/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.8308 - acc: 0.9578 - val_loss: 12.9645 - val_acc: 0.9527\n",
      "Epoch 94/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.5464 - acc: 0.9557 - val_loss: 13.2889 - val_acc: 0.9497\n",
      "Epoch 95/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.9547 - acc: 0.9560 - val_loss: 13.3095 - val_acc: 0.9503\n",
      "Epoch 96/500\n",
      "6680/6680 [==============================] - 151s - loss: 9.5372 - acc: 0.9525 - val_loss: 12.5491 - val_acc: 0.9491\n",
      "Epoch 97/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.3954 - acc: 0.9548 - val_loss: 13.1291 - val_acc: 0.9581\n",
      "Epoch 98/500\n",
      "6680/6680 [==============================] - 151s - loss: 9.4044 - acc: 0.9560 - val_loss: 12.2844 - val_acc: 0.9479\n",
      "Epoch 99/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.5138 - acc: 0.9564 - val_loss: 12.3977 - val_acc: 0.9491\n",
      "Epoch 100/500\n",
      "6680/6680 [==============================] - 151s - loss: 9.1869 - acc: 0.9533 - val_loss: 12.5259 - val_acc: 0.9455\n",
      "Epoch 101/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.2362 - acc: 0.9575 - val_loss: 15.1841 - val_acc: 0.9533\n",
      "Epoch 102/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.2526 - acc: 0.9546 - val_loss: 13.3616 - val_acc: 0.9521\n",
      "Epoch 103/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.4448 - acc: 0.9534 - val_loss: 11.8634 - val_acc: 0.9533\n",
      "Epoch 104/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.1120 - acc: 0.9531 - val_loss: 12.5908 - val_acc: 0.9509\n",
      "Epoch 105/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.9293 - acc: 0.9587 - val_loss: 12.3193 - val_acc: 0.9575\n",
      "Epoch 106/500\n",
      "6680/6680 [==============================] - 151s - loss: 9.1365 - acc: 0.9557 - val_loss: 11.7018 - val_acc: 0.9623\n",
      "Epoch 107/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.9004 - acc: 0.9545 - val_loss: 12.9134 - val_acc: 0.9569\n",
      "Epoch 108/500\n",
      "6680/6680 [==============================] - 151s - loss: 9.0125 - acc: 0.9557 - val_loss: 12.4231 - val_acc: 0.9485\n",
      "Epoch 109/500\n",
      "6680/6680 [==============================] - 151s - loss: 9.1355 - acc: 0.9507 - val_loss: 12.1122 - val_acc: 0.9491\n",
      "Epoch 110/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.1148 - acc: 0.9531 - val_loss: 12.1946 - val_acc: 0.9617\n",
      "Epoch 111/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.9248 - acc: 0.9525 - val_loss: 12.6184 - val_acc: 0.9575\n",
      "Epoch 112/500\n",
      "6680/6680 [==============================] - 150s - loss: 9.0167 - acc: 0.9540 - val_loss: 11.8667 - val_acc: 0.9461\n",
      "Epoch 113/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.9211 - acc: 0.9566 - val_loss: 13.1223 - val_acc: 0.9395\n",
      "Epoch 114/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.7055 - acc: 0.9543 - val_loss: 13.1363 - val_acc: 0.9497\n",
      "Epoch 115/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.3604 - acc: 0.9549 - val_loss: 12.7005 - val_acc: 0.9293\n",
      "Epoch 116/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.8401 - acc: 0.9554 - val_loss: 12.7919 - val_acc: 0.9521\n",
      "Epoch 117/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.6376 - acc: 0.9525 - val_loss: 13.3130 - val_acc: 0.9527\n",
      "Epoch 118/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.8980 - acc: 0.9554 - val_loss: 12.6127 - val_acc: 0.9461\n",
      "Epoch 119/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.6381 - acc: 0.9587 - val_loss: 12.5914 - val_acc: 0.9425\n",
      "Epoch 120/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.4743 - acc: 0.9566 - val_loss: 13.2195 - val_acc: 0.9527\n",
      "Epoch 121/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.6474 - acc: 0.9567 - val_loss: 12.5210 - val_acc: 0.9563\n",
      "Epoch 122/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.4340 - acc: 0.9596 - val_loss: 13.8275 - val_acc: 0.9629\n",
      "Epoch 123/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.6235 - acc: 0.9573 - val_loss: 12.2121 - val_acc: 0.9563\n",
      "Epoch 124/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.7993 - acc: 0.9578 - val_loss: 13.7787 - val_acc: 0.9485\n",
      "Epoch 125/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 151s - loss: 8.6648 - acc: 0.9581 - val_loss: 12.1344 - val_acc: 0.9557\n",
      "Epoch 126/500\n",
      "6680/6680 [==============================] - 151s - loss: 8.5255 - acc: 0.9566 - val_loss: 12.3773 - val_acc: 0.9497\n",
      "Epoch 127/500\n",
      "6680/6680 [==============================] - 151s - loss: 8.2116 - acc: 0.9593 - val_loss: 11.6752 - val_acc: 0.9497\n",
      "Epoch 128/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.5617 - acc: 0.9570 - val_loss: 13.1527 - val_acc: 0.9509\n",
      "Epoch 129/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.8283 - acc: 0.9569 - val_loss: 14.4057 - val_acc: 0.9479\n",
      "Epoch 130/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.3414 - acc: 0.9578 - val_loss: 11.8600 - val_acc: 0.9509\n",
      "Epoch 131/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.3355 - acc: 0.9582 - val_loss: 12.1971 - val_acc: 0.9449\n",
      "Epoch 132/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.2256 - acc: 0.9594 - val_loss: 13.9751 - val_acc: 0.9533\n",
      "Epoch 133/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.3097 - acc: 0.9567 - val_loss: 11.8624 - val_acc: 0.9575\n",
      "Epoch 134/500\n",
      "6680/6680 [==============================] - 151s - loss: 8.4260 - acc: 0.9561 - val_loss: 11.9753 - val_acc: 0.9635\n",
      "Epoch 135/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.1571 - acc: 0.9605 - val_loss: 11.9363 - val_acc: 0.9659\n",
      "Epoch 136/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.3083 - acc: 0.9585 - val_loss: 12.6122 - val_acc: 0.9563\n",
      "Epoch 137/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.9443 - acc: 0.9581 - val_loss: 11.8542 - val_acc: 0.9443\n",
      "Epoch 138/500\n",
      "6680/6680 [==============================] - 151s - loss: 8.0873 - acc: 0.9563 - val_loss: 12.5468 - val_acc: 0.9491\n",
      "Epoch 139/500\n",
      "6680/6680 [==============================] - 151s - loss: 8.0615 - acc: 0.9570 - val_loss: 11.8539 - val_acc: 0.9557\n",
      "Epoch 140/500\n",
      "6680/6680 [==============================] - 151s - loss: 8.2528 - acc: 0.9573 - val_loss: 13.9830 - val_acc: 0.9533\n",
      "Epoch 141/500\n",
      "6680/6680 [==============================] - 151s - loss: 7.8798 - acc: 0.9563 - val_loss: 11.4499 - val_acc: 0.9569\n",
      "Epoch 142/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.0509 - acc: 0.9581 - val_loss: 14.4991 - val_acc: 0.9575\n",
      "Epoch 143/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.9132 - acc: 0.9582 - val_loss: 11.6913 - val_acc: 0.9497\n",
      "Epoch 144/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.8339 - acc: 0.9579 - val_loss: 12.9053 - val_acc: 0.9485\n",
      "Epoch 145/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.8536 - acc: 0.9576 - val_loss: 11.5909 - val_acc: 0.9635\n",
      "Epoch 146/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.7448 - acc: 0.9567 - val_loss: 13.1307 - val_acc: 0.9533\n",
      "Epoch 147/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.6665 - acc: 0.9584 - val_loss: 12.8740 - val_acc: 0.9467\n",
      "Epoch 148/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.9039 - acc: 0.9584 - val_loss: 11.5403 - val_acc: 0.9581\n",
      "Epoch 149/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.6635 - acc: 0.9573 - val_loss: 13.4742 - val_acc: 0.9491\n",
      "Epoch 150/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.7965 - acc: 0.9552 - val_loss: 12.4389 - val_acc: 0.9497\n",
      "Epoch 151/500\n",
      "6680/6680 [==============================] - 150s - loss: 8.0369 - acc: 0.9605 - val_loss: 12.3775 - val_acc: 0.9539\n",
      "Epoch 152/500\n",
      "6680/6680 [==============================] - 151s - loss: 7.5401 - acc: 0.9590 - val_loss: 11.0853 - val_acc: 0.9497\n",
      "Epoch 153/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.6881 - acc: 0.9573 - val_loss: 12.9316 - val_acc: 0.9575\n",
      "Epoch 154/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.7397 - acc: 0.9617 - val_loss: 12.1322 - val_acc: 0.9503\n",
      "Epoch 155/500\n",
      "6680/6680 [==============================] - 151s - loss: 7.9275 - acc: 0.9573 - val_loss: 12.4848 - val_acc: 0.9587\n",
      "Epoch 156/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.8705 - acc: 0.9581 - val_loss: 11.9264 - val_acc: 0.9545\n",
      "Epoch 157/500\n",
      "6680/6680 [==============================] - 151s - loss: 7.5755 - acc: 0.9602 - val_loss: 15.0043 - val_acc: 0.9503\n",
      "Epoch 158/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.7702 - acc: 0.9566 - val_loss: 11.6938 - val_acc: 0.9341\n",
      "Epoch 159/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.4990 - acc: 0.9596 - val_loss: 12.8538 - val_acc: 0.9551\n",
      "Epoch 160/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.3103 - acc: 0.9543 - val_loss: 11.3492 - val_acc: 0.9599\n",
      "Epoch 161/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.4924 - acc: 0.9564 - val_loss: 12.8168 - val_acc: 0.9461\n",
      "Epoch 162/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.5751 - acc: 0.9602 - val_loss: 12.8808 - val_acc: 0.9485\n",
      "Epoch 163/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.5245 - acc: 0.9624 - val_loss: 11.1797 - val_acc: 0.9437\n",
      "Epoch 164/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.2792 - acc: 0.9590 - val_loss: 12.3705 - val_acc: 0.9575\n",
      "Epoch 165/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.5529 - acc: 0.9620 - val_loss: 12.2351 - val_acc: 0.9605\n",
      "Epoch 166/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.3497 - acc: 0.9639 - val_loss: 13.1519 - val_acc: 0.9503\n",
      "Epoch 167/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.2978 - acc: 0.9576 - val_loss: 11.1779 - val_acc: 0.9611\n",
      "Epoch 168/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.3275 - acc: 0.9605 - val_loss: 12.6803 - val_acc: 0.9533\n",
      "Epoch 169/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.3058 - acc: 0.9603 - val_loss: 11.3267 - val_acc: 0.9515\n",
      "Epoch 170/500\n",
      "6680/6680 [==============================] - 151s - loss: 7.5724 - acc: 0.9632 - val_loss: 10.9463 - val_acc: 0.9491\n",
      "Epoch 171/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.4114 - acc: 0.9575 - val_loss: 11.3900 - val_acc: 0.9563\n",
      "Epoch 172/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.4035 - acc: 0.9570 - val_loss: 11.5541 - val_acc: 0.9521\n",
      "Epoch 173/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.0935 - acc: 0.9582 - val_loss: 12.7370 - val_acc: 0.9473\n",
      "Epoch 174/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.4257 - acc: 0.9579 - val_loss: 12.0706 - val_acc: 0.9467\n",
      "Epoch 175/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.0888 - acc: 0.9560 - val_loss: 12.3652 - val_acc: 0.9539\n",
      "Epoch 176/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.4372 - acc: 0.9603 - val_loss: 11.8810 - val_acc: 0.9557\n",
      "Epoch 177/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.1772 - acc: 0.9623 - val_loss: 12.1661 - val_acc: 0.9527\n",
      "Epoch 178/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.2043 - acc: 0.9561 - val_loss: 11.5329 - val_acc: 0.9521\n",
      "Epoch 179/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.4000 - acc: 0.9585 - val_loss: 13.7272 - val_acc: 0.9545\n",
      "Epoch 180/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.9548 - acc: 0.9567 - val_loss: 11.5269 - val_acc: 0.9551\n",
      "Epoch 181/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.3104 - acc: 0.9573 - val_loss: 13.2399 - val_acc: 0.9389\n",
      "Epoch 182/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.9669 - acc: 0.9597 - val_loss: 11.3554 - val_acc: 0.9539\n",
      "Epoch 183/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.2264 - acc: 0.9567 - val_loss: 12.6205 - val_acc: 0.9653\n",
      "Epoch 184/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.0194 - acc: 0.9575 - val_loss: 11.4772 - val_acc: 0.9557\n",
      "Epoch 185/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.8891 - acc: 0.9581 - val_loss: 13.4135 - val_acc: 0.9515\n",
      "Epoch 186/500\n",
      "6680/6680 [==============================] - 151s - loss: 6.7571 - acc: 0.9581 - val_loss: 10.8889 - val_acc: 0.9533\n",
      "Epoch 187/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 150s - loss: 6.9926 - acc: 0.9608 - val_loss: 11.0124 - val_acc: 0.9539\n",
      "Epoch 188/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.0236 - acc: 0.9599 - val_loss: 11.2547 - val_acc: 0.9521\n",
      "Epoch 189/500\n",
      "6680/6680 [==============================] - 151s - loss: 6.9606 - acc: 0.9578 - val_loss: 12.4925 - val_acc: 0.9581\n",
      "Epoch 190/500\n",
      "6680/6680 [==============================] - 151s - loss: 6.9254 - acc: 0.9636 - val_loss: 11.4584 - val_acc: 0.9539\n",
      "Epoch 191/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.8011 - acc: 0.9587 - val_loss: 11.1725 - val_acc: 0.9479\n",
      "Epoch 192/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.9569 - acc: 0.9594 - val_loss: 11.3026 - val_acc: 0.9449\n",
      "Epoch 193/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.9841 - acc: 0.9594 - val_loss: 11.3748 - val_acc: 0.9581\n",
      "Epoch 194/500\n",
      "6680/6680 [==============================] - 150s - loss: 7.0867 - acc: 0.9593 - val_loss: 10.8964 - val_acc: 0.9551\n",
      "Epoch 195/500\n",
      "6680/6680 [==============================] - 151s - loss: 6.8342 - acc: 0.9611 - val_loss: 12.4415 - val_acc: 0.9467\n",
      "Epoch 196/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.9680 - acc: 0.9570 - val_loss: 11.1930 - val_acc: 0.9617\n",
      "Epoch 197/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.6509 - acc: 0.9561 - val_loss: 10.9121 - val_acc: 0.9575\n",
      "Epoch 198/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.8851 - acc: 0.9584 - val_loss: 11.0572 - val_acc: 0.9605\n",
      "Epoch 199/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.9596 - acc: 0.9611 - val_loss: 11.6435 - val_acc: 0.9509\n",
      "Epoch 200/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.7662 - acc: 0.9612 - val_loss: 11.3320 - val_acc: 0.9467\n",
      "Epoch 201/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.8502 - acc: 0.9587 - val_loss: 11.5638 - val_acc: 0.9503\n",
      "Epoch 202/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.8399 - acc: 0.9569 - val_loss: 11.5528 - val_acc: 0.9575\n",
      "Epoch 203/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.7665 - acc: 0.9591 - val_loss: 11.4563 - val_acc: 0.9575\n",
      "Epoch 204/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.7008 - acc: 0.9605 - val_loss: 11.4171 - val_acc: 0.9605\n",
      "Epoch 205/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.6844 - acc: 0.9603 - val_loss: 11.3973 - val_acc: 0.9593\n",
      "Epoch 206/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.7584 - acc: 0.9593 - val_loss: 12.0984 - val_acc: 0.9485\n",
      "Epoch 207/500\n",
      "6680/6680 [==============================] - 150s - loss: 6.9797 - acc: 0.9599 - val_loss: 12.5985 - val_acc: 0.9563\n",
      "1670/1670 [==============================] - 9s     \n",
      "Evaluation results:\n",
      "Loss: 12.59853822628181\n",
      "Accuracy: 0.9562874251497006\n"
     ]
    }
   ],
   "source": [
    "# 조기종료\n",
    "early_stopping = EarlyStopping(patience =20, monitor = 'val_loss')\n",
    "checkpoint_cb = ModelCheckpoint(\"h5/cnn_best_model_20240416_태양정보추가_방위각변환안함.h5\", monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "# Training the fusion model with early stopping\n",
    "history = fusion_model.fit([image_train, sum_info_train], illum_train, \n",
    "                           validation_data=([image_val, sum_info_val], illum_val), \n",
    "                           epochs=500, batch_size=4,\n",
    "                           callbacks=[early_stopping, checkpoint_cb])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluation_results = fusion_model.evaluate([image_val, sum_info_val], illum_val)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\")\n",
    "print(\"Loss:\", evaluation_results[0])\n",
    "print(\"Accuracy:\", evaluation_results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c46f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
